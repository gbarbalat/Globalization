---
title: "sAP Globalization"
author: "Guillaume Barbalat"
date: "17/04/2022"
output: html_document
---

# Step 0 specify the research question

A recent study published in the Lancet has demonstrated the impact of globalization on child malnutrition in LMIC. Socio-economic factors (unemployment, inequalities) and cultural traits (my study!!) have also been associated to mental disorders. These parameters are also largely linked to globalization. 

Hence the research question:

- P: In all countries around the world vs. European countries
- I: what is the direct impact of globalization
- C: With different levels of globalization
- O: on the prevalence and incidence of mental and behavioural disorders (that include alcohol, drug issues and self-harm).
- S (study design): observational cross-sectionnal (STROBE guidelines)

Heterogeneous treatment effects: age, sex

# Step 1: specify the causal model

Our DAG represents a typical W-->A-->Y<--W
In addition, we investigate the heterogeneous treatment effect of globalization on mental disorders over various age and sex strata.
Note that our `Y_0` variable is that of year 1990, the more distant year to both our calculation of globalization index and 2019 DALYs (so that values are not too correlated).


# Step 2: Specify the data and its link to the causal model

Master database is the GBD study 2019.

```{r warning=FALSE, message=FALSE}
#read_clean.R, which produces a final_matrix data.frame
#check out lines 40
rm(list=ls())
library(dplyr)
library(tmle)
#gives final matrix
loaded_data=load(file="G:/PROJECTS/2022/sAP Globalization/IPV_Astd_DALYs.RData")
title_here=unique(final_matrix$cause)

#only small locations (MOnaco, Miu, Vanuatu, Taiwan etc ...) are missing
Y_0_matrix<-final_matrix %>%
  filter(year==1990) %>%
  select(c(location,age,sex,Y)) %>%
  #rename(Y_0=Y) %>%
  group_by(location) %>%
  mutate(Y_0=sum(Y)) %>%
  select(c(location,Y_0)) %>%
  ungroup()

A_W_matrix <- final_matrix %>% #it should be A_W_matrix
  filter(year==2018) %>% #2018
  select(-c(Y)) 

Y_matrix <- final_matrix %>%
  filter(year==2019) %>%
  select(c(location,age,sex,Y)) %>%
  group_by(location) %>%
  mutate(Y=sum(Y)) %>%
  select(c(location,Y)) %>%
  ungroup()

#binearize KOFGI
#log Y
#numerize V strata
final_matrix_2019 <- Y_matrix %>%
  left_join(Y_0_matrix,by=c("location")) %>%#  left_join(Y_0_matrix,by=c("location","age","sex")) %>%

  left_join(A_W_matrix,by=c("location")) %>%#  left_join(A_W_matrix,by=c("location","age","sex")) %>%

  #left_join(data_pop_struct,by=c("location","age","sex","year")) %>%
  mutate(bin_KOFGI=case_when(
    KOFGI>=median(KOFGI, na.rm=TRUE) ~ 1,
    KOFGI<median(KOFGI, na.rm=TRUE) ~ 0 ),
    log_Y=log(Y),
    log_Y_0=log(Y_0),
    #age_num = as.numeric(factor(age, levels = as.character(unique(age)))),
    #sex_num = as.numeric(factor(sex, levels = as.character(unique(sex))))
  )

#run a complete cases function to identify participating locations
final_matrix_2019=final_matrix_2019[complete.cases(final_matrix_2019),]

#only quality data sources, binearize KOFGI and add an index centering KOFGI in each group
final_matrix_quality_2019 <- final_matrix_2019 %>% 
  filter(Quality>=3) %>%
  mutate(Quality = Quality - 3,
         bin_KOFGI=case_when(
         KOFGI>median(KOFGI, na.rm=TRUE) ~ 1,
         KOFGI<=median(KOFGI, na.rm=TRUE) ~ 0 )
         ) %>%
  group_by(bin_KOFGI) %>%
  mutate(KOFGI_ctrd=scale(KOFGI,scale = TRUE)) %>%
  relocate(KOFGI_ctrd, .after = KOFGI) %>%
  ungroup()

median(final_matrix_quality_2019$KOFGI)
summary(final_matrix_quality_2019[with(final_matrix_quality_2019,bin_KOFGI==0),"KOFGI"])
summary(final_matrix_quality_2019[with(final_matrix_quality_2019,bin_KOFGI==1),"KOFGI"])
```

## table 1
```{r warning=FALSE, message=FALSE}
##########
#Table 1
##########
library(table1)
my.render.cont <- function(x) {
  with(stats.apply.rounding(stats.default(x), digits=2, round.integers = FALSE), 
       #c("","Mean (SD) \n IQR Min-Max"=sprintf("%s (&plusmn; %s) %s %s %s", MEAN, SD, IQR, MIN, MAX)))
       c("","Mean (SD)"=sprintf("%s (&plusmn %s)",MEAN, SD),
         "\n Range"= sprintf("%s-%s",MIN, MAX)))
}
my.render.cont <- function(x) {
  with(stats.apply.rounding(stats.default(x), digits=2, round.integers = FALSE), 
       #c("","Mean (SD) \n IQR Min-Max"=sprintf("%s (&plusmn; %s) %s %s %s", MEAN, SD, IQR, MIN, MAX)))
       c("","Mean (SD) \n Range"=sprintf("%s (&plusmn %s) %s-%s",MEAN, SD, MIN, MAX)))
}

glob_table<-table1(~ Y + Y_0+ unemploy + SDI + urban + unhappy + CSA + p90p100 + haqi
                   | bin_KOFGI#*sex or age
                   ,render.continuous=my.render.cont
                   ,data=final_matrix_quality_2019)

location_table <- table1(~ location
                         | bin_KOFGI
                         ,data=final_matrix_quality_2019)
glob_table
location_table

#graphs
library(ggplot2)
g1<-ggplot(final_matrix_quality_2019,
           aes(y=Y,x=as.factor(bin_KOFGI),colour=as.factor(bin_KOFGI))) +
  geom_boxplot() +
  labs(x='Globalization',y="2019 DALYs",title = title_here)+
  theme(legend.position = "none",
        panel.background = element_rect(fill = "white",colour = "black"),
        strip.background = element_rect(colour = "black", fill = "white"),
        axis.title.x = element_text(face = "bold",size=16),
        axis.title.y = element_text(face = "bold",size=16),#element_text(face = "bold",size=16),#element_blank()
        axis.text.y=element_text(face = "bold",size=12),
        axis.text.x = element_text(colour = "black", size=12),# 
        plot.title = element_text(colour = "black",face="bold", size=20)#,hjust = 0.5)#family, 
  )
  #+facet_grid(cols = vars(age),rows = vars(sex))
g1

g2<-ggplot(final_matrix_quality_2019,
           aes(y=Y_0,x=as.factor(bin_KOFGI),colour=as.factor(bin_KOFGI))) +
  geom_boxplot() +
  labs(x='Globalization',y="1990 DALYs",title = title_here)+
  theme(legend.position = "none",
        panel.background = element_rect(fill = "white",colour = "black"),
        strip.background = element_rect(colour = "black", fill = "white"),
        axis.title.x = element_text(face = "bold",size=16),
        axis.title.y = element_text(face = "bold",size=16),#element_text(face = "bold",size=16),#element_blank()
        axis.text.y=element_text(face = "bold",size=12),
        axis.text.x = element_text(colour = "black", size=12),# 
        plot.title = element_text(colour = "black",face="bold", size=20)#,hjust = 0.5)#family, 
  )
  #+facet_grid(cols = vars(age),rows = vars(sex))
g2
```

NPSEM

# Step 3: Specify the counterfactuals and the target causal parameter

We're interested in the effect of a high level of globalization ($A=1$) vs. a low level of globalization ($A=0$) on the prevalence of mental disorders ($Y$), over various age and sex groups ($V$). 
Globalization is defined

We define $Y_1$ as the average prevalence if, possibly contrary to the fact, all countries were under a high level of globalization.
We define $Y_0$ as the average prevalence if, possibly contrary to the fact, all countries were under a high level of globalization.

We define a marginal structural model $m$ where the various age and sex strata $V$ are taken into account in the same model. This would allow the modeling of heterogeneous treatment effect, or said differently, whether $A$ has a different impact on $Y$ among various levels of $V$.  
Our aim is to compute $\beta$ such that 

$$E^*[Y_a]=m(a,v|β)=\beta_0+\beta_1a+\beta_2v_1+\beta_3v_2+\beta_4av_1+\beta_5av_2$$

We place particular emphasis onto $\beta_1, \beta_4$ and $\beta_5$

# Step 4: Identifiability


The goal of identification is to write our causal estimate as a property of the counterfactuals’ distribution using only the observed data. To do this, we must satisfy the main assumptions of temporality, consistency, conditional randomization and positivity.


1- Temporality

This assumption stipulates that baseline covariates W (measured in 2018, except for baseline outcome measure in 1990) should happen before allocation to exposure A (A=1 vs. A=0) , which should itself happen before the occurence of outcome Y (measured in 2019).


2- Consistency

The consistency assumption stipulates that there is only one version of exposure As mentioned in the main text, the globalization index is consistent across locations, yet is composed by sub-indices which are not. To improve consistency of exposure, we chose to binearize our exposure to A=0 for locations that were not  


3- Conditional randomization 

This assumption means that the so-called “back-door criterion” is satisfied conditional on some strata of covariates; in other words, in our DAG all paths between the treatment and the outcome are blocked after conditioning on some set of covariates. Not considering background factors, conditioning on all baseline covariates should be sufficient to block all paths between globalization (A) and 2019 MBD (Y). 

Yet, the latter would be true if background factors were uncorrelated. Specifically, for the backdoor path criterion to be satisfied, we would need to assume no interference or contagion between states after having accounted for baseline covariates. This means that, for instance, geo-spatial historical and cultural factors linking countries and relating Globalization to 2019 MBD DALYs would need to be entirely accounted for by our baseline covariates. As briefly mentioned in our Discussion section (main text), and though baseline covariates cover a wide range of socio-economic factors that are linked to geo-spatial historical and cultural accounts, the conditional randomization assumption might not be entirely verified in our case. 


4- Positivity

The positivity assumption is satisfied when, in the data, every combination of exposure and confounders has a nonzero (ie, positive) probability of receiving both levels of exposure. Otherwise formulated, in our case the positivity assumption stipulates that, given their characteristics, every country has some chance of being exposed and not exposed to globalization 
Diagnostics that concern violations of positivity are typically provided by histograms of the inverse probability of treatment given baseline covariates (a.k.a. weights). Ideally, weights should range around a value of 1, and departure from the positivity assumption is associated with very large weights (e.g. > 500 (Platt, Delaney, & Suissa, 2012)). 
Here, the propensity score (the probability of being exposed given a set of baseline covariates) was robustly estimated using machine learning algorithms. Histograms of weights were provided below. In general, weights were below 40 for all age/sex strata.

Sensitivity analysis: 

- take into account various measures of globalization.
- sensitivity analysis based on weights' trimming

# Step 5: Specify the statistical model and statistical estimand

For the MSM, our statistical model aimed at estimating  in the following statistical equation: 
$$E[Y|A,W,V]=\beta_0+\beta_1A+\beta_2V_1+\beta_3V_2+\beta_4A*V_1+\beta_5A*V_2$$



# Step 6: Estimate

## Step 6a: Estimate using `tmle`

```{r eval=TRUE, warning=FALSE, message=FALSE}
design_matrix <- final_matrix_quality_2019 # 
all_libraries= c("SL.mean","SL.glm","SL.step.forward"
                 #,"SL.step.interaction" too long, not sure it works
                         ,"SL.glmnet","SL.earth","SL.gam"
                         ,"SL.ranger"#, "SL.xgboost"
                         )
little_libraries=c("SL.mean"
                   #,"SL.step.forward"#,"SL.gam"
                   )

system.time(
  tmle_baseline <- tmle(Y=design_matrix$log_Y,
                     A=design_matrix$bin_KOFGI,
                     W=select(design_matrix,c(unemploy:haqi #unemploy or KOFGI_ctrd
                                             #,age_num,sex_num #no!!!
                                             #,c(location_Albania:location_Zimbabwe)#location FE 
                                             #,c(year_1992:year_2019)# year FE
                                             ,Quality,log_Y_0#,pct_pop
                                             )),
        Q.SL.library = all_libraries,#all_libraries, little_libraries
        g.SL.library = all_libraries,#all_libraries,# little_libraries
        verbose=TRUE,
        V = 20
        )
  )
tmle_baseline
```

## interlude: redo analysis without observations based on weights
```{r}
#design_matrix <- final_matrix_quality_2019 # 
#load("Dep_results_QualitySet.RData")
#tmle_baseline=results

pct_95=quantile(1/tmle_baseline$g$g1W,c(0.05,0.95))
exclude_wt=c(100,500,"pct_95","ub40","ub100","dist_binA","outliers_A")

```

## Step 6b: Sensitivity analysis - trimming/truncating weights
```{r eval=TRUE,warning=FALSE, message=FALSE}
design_matrix_wt=data.frame(design_matrix,wt=1/tmle_baseline$g$g1W)

#do_parallel
{
library(doParallel)
num_cores <- detectCores()
cl <- makeCluster(length(exclude_wt))
clusterEvalQ(cl,c(library(tmle),library(dplyr)))
clusterExport(cl, list("all_libraries"))

try_this <- function(x,design_matrix_wt, exclude_wt,pct_95) {

if (exclude_wt[x]=="outliers_A") {
  #take off "outliers" in terms of KOFGI
  exclude <- design_matrix_wt %>%
    filter(KOFGI<54)%>%
    select(c(location))
  
  gbound = 5/sqrt(nrow(design_matrix_wt))/log(nrow(design_matrix_wt))
  
  } else if (exclude_wt[x]=="dist_binA") {
    #rescale KOFGI_ctrd in the new design_matrix
    design_matrix_wt <- design_matrix_wt %>%
        relocate(KOFGI_ctrd, .after = unemploy) 
      exclude<-data.frame(location=NA)
    
    gbound = 5/sqrt(nrow(design_matrix_wt))/log(nrow(design_matrix_wt))

    } else if(exclude_wt[x]=="pct_95") {
    #take off "outliers" in terms of wt
    exclude <- design_matrix_wt %>%
      filter(wt>pct_95[2] | wt<pct_95[1]) %>% 
      select(c(location))
    
    gbound = 5/sqrt(nrow(design_matrix_wt))/log(nrow(design_matrix_wt))
    
    } else if(exclude_wt[x]=="ub40") {
      exclude<-data.frame(location=NA)
      gbound = 1/40
    
    } else if(exclude_wt[x]=="ub100") {
      exclude<-data.frame(location=NA)
      gbound = 1/100
    
    } else {
    #take off "outliers" in terms of wt
    exclude <- design_matrix_wt %>%
    filter(wt>exclude_wt[x]) %>%
    select(c(location))
    
    gbound = 5/sqrt(nrow(design_matrix_wt))/log(nrow(design_matrix_wt))

    }

design_matrix_sensit <- design_matrix_wt %>%
  anti_join(exclude,by=c("location")) 

tmle_sensit <- tmle(Y=design_matrix_sensit$log_Y,
                    A=design_matrix_sensit$bin_KOFGI,
                    W=select(design_matrix_sensit,c(unemploy:haqi #unemploy or KOFGI_ctrd
                                             #,age_num,sex_num #no!!!
                                             #,c(location_Albania:location_Zimbabwe)#location FE 
                                             #,c(year_1992:year_2019)# year FE
                                             ,Quality,log_Y_0#,pct_pop
                                             )),
                    Q.SL.library = all_libraries,#all_libraries, little_libraries
                    g.SL.library = all_libraries,#all_libraries,# little_libraries
                    verbose=TRUE,
                    V = 20,
                    gbound=gbound
        )
  
  return(list(tmle_sensit,exclude))
}

system.time(anal_sensit <-clusterApply(cl,1:length(exclude_wt),try_this,design_matrix_wt, exclude_wt,pct_95))

save(tmle_baseline, anal_sensit,
     file=paste0(title_here,"_results_QualitySet.RData")) 

stopCluster(cl)
}
```

## Step 6c: Dx

```{r echo=FALSE}
diagnose <- function(dx, exclude=NULL) {
#SuperLearner coeff
print(dx$Qinit$coef)
print(dx$Qinit$Rsq)

print(dx$g$coef)
print(dx$g$AUC)
print(dx$gbound)
print(dx$W.retained)


#stb weights
hist(1/dx$g$g1W,
     main="Histogram of weights (inverse of propensity scores)",
     xlab="weight value")
print(summary(1/dx$g$g1W))

print(exclude)
print(paste0("Exclusion of ",nrow(exclude)," observations"));
print(paste0("Exclusion of ",length(unique(exclude$location))," locations"));
print(unique(exclude$location))
}

print("Baseline model");diagnose(tmle_baseline)

load(file=paste0(title_here,"_results_QualitySet.RData"))
for (x in 1:length(exclude_wt)) {
print(paste0("Trimming(nb/pct) weights/Truncating (ub) weights/include W_dist_binA/exclude outliers_A - ",exclude_wt[x]));
diagnose(anal_sensit[[x]][[1]], anal_sensit[[x]][[2]])
}
```

## Step 6d: results (tables)

```{r echo=FALSE}
print("Baseline model");tmle_baseline

for (x in 1:length(exclude_wt)) {
print(paste0("Trimming(nb/pct) weights/Truncating (ub) weights/include W_dist_binA/exclude outliers_A - ",exclude_wt[x]));
print(anal_sensit[[x]][[1]])
}
```